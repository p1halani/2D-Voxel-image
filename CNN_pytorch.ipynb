{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import seed\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import fbeta_score, precision_recall_fscore_support\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_num_threads(5)\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "if not os.path.exists('Features'):\n",
    "    os.makedirs('Features')\n",
    "\n",
    "\"\"\"\n",
    "voxel_type can take 4 values - None, bp, cc, mf\n",
    "\"\"\"\n",
    "\n",
    "voxel_type = 'None'\n",
    "train_dir = os.path.join('..',f'pictures/train/2/{voxel_type}')\n",
    "labels_csv= os.path.join('../labels/struct','new_2.csv')\n",
    "resnet_weights_path = 'Resnet50.pth'\n",
    "weights_path = f'weights_{voxel_type}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed=27):\n",
    "    \"\"\"https://pytorch.org/docs/stable/notes/randomness.html\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 20\n",
    "PERCENTILE = 99.7\n",
    "LEARNING_RATE = 0.00001\n",
    "DISABLE_TQDM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(labels_csv)\n",
    "attribute_dict = dict(zip(df.accession_no,df.labels))\n",
    "output_dim = 439\n",
    "# del df,labels_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(train_csv)\n",
    "# labels_dict = dict(zip(df.id,df.attribute_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add more transforms here\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class ImageData(data.Dataset):\n",
    "    def __init__(self,df,dirpath,transform,test = False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.dirpath = dirpath\n",
    "        self.conv_to_tensor = transform\n",
    "        #image data \n",
    "        if not self.test:\n",
    "            self.image_arr = np.asarray(str(self.dirpath)+'/'+self.df.iloc[:, 0])\n",
    "        else:\n",
    "            self.image_arr = np.asarray(str(self.dirpath)+'/'+self.df.iloc[:, 0])\n",
    "        \n",
    "        #labels data\n",
    "        if not self.test:\n",
    "             self.label_df = self.df.iloc[:,1]\n",
    "        \n",
    "        # Calculate length of df\n",
    "        self.data_len = len(self.df.index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_arr[idx]\n",
    "        img = Image.open(image_name)\n",
    "        img = img.convert(mode = 'RGB')\n",
    "        img_tensor = self.conv_to_tensor(img)\n",
    "        if not self.test:\n",
    "            image_labels = self.label_df[idx]\n",
    "            label_tensor = torch.zeros((1, output_dim))\n",
    "            image_labels = [int(x) for x in image_labels.split(',')]\n",
    "            for i, label in enumerate(image_labels):\n",
    "                label_tensor[0, i] = label\n",
    "            image_label = torch.tensor(label_tensor,dtype= torch.float32)\n",
    "            return (img_tensor,image_label.squeeze())\n",
    "        return (img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation_Data Length: 2064\n",
      " Train_Data Length: 8255\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(labels_csv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df,val_df = train_test_split(df, test_size=0.20)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "print(f\"Validation_Data Length: {len(val_df)}\\n Train_Data Length: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train dataset\n",
    "train_dataset = ImageData(train_df,train_dir,data_transforms)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = ImageData(val_df,train_dir,data_transforms)\n",
    "val_loader = data.DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train':train_loader, 'val':val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features: torch.Size([50, 3, 224, 224])\n",
      "Train Labels: torch.Size([50, 439])\n",
      "\n",
      "Validation Features: torch.Size([50, 3, 224, 224])\n",
      "Validation Labels: torch.Size([50, 439])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_loader))\n",
    "print(f'Train Features: {features.shape}\\nTrain Labels: {labels.shape}')\n",
    "print()\n",
    "features, labels = next(iter(val_loader))\n",
    "print(f'Validation Features: {features.shape}\\nValidation Labels: {labels.shape}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_cls = models.resnet50()\n",
    "resnet_cls.load_state_dict(torch.load(resnet_weights_path))\n",
    "\n",
    "class AvgPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, x.shape[2:])\n",
    "    \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self,num_outputs):\n",
    "        super(ResNet50,self).__init__()\n",
    "        self.resnet = resnet_cls\n",
    "        layer4 = self.resnet.layer4\n",
    "        self.resnet.layer4 = nn.Sequential(nn.Dropout(0.5), layer4)\n",
    "        self.resnet.avgpool = AvgPool()\n",
    "        self.resnet.fc = nn.Linear(2048, 1024)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, num_outputs)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.resnet.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        for param in self.fc1.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        for param in self.bn1.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        for param in self.fc2.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "#         for param in self.resnet.classifier.parameters():\n",
    "#             param.requires_grad = True\n",
    "            \n",
    "    def forward(self,x):\n",
    "        out = self.bn1(self.resnet(x))\n",
    "        out = F.sigmoid(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "NeuralNet = ResNet50(num_outputs = output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet50(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Dropout(p=0.5)\n",
       "      (1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool()\n",
       "    (fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=439, bias=True)\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25,983,479 total parameters.\n",
      "17,440,183 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in NeuralNet.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in NeuralNet.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "training examples:  8255\n",
      "batch size:  50\n",
      "batches available:  166\n",
      "\n",
      "VALIDATION\n",
      "validation examples:  2064\n",
      "batch size:  50\n",
      "batches available:  42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"training examples: \",len(train_dataset))\n",
    "print(\"batch size: \",BATCH_SIZE)\n",
    "print(\"batches available: \",len(train_loader))\n",
    "print()\n",
    "print(\"VALIDATION\")\n",
    "print(\"validation examples: \",len(val_dataset))\n",
    "print(\"batch size: \",BATCH_SIZE)\n",
    "print(\"batches available: \",len(val_loader))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNet = NeuralNet.to(device)\n",
    "optimizer = optim.Adam(NeuralNet.parameters(),lr = LEARNING_RATE)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience = 2)\n",
    "best_loss = np.inf\n",
    "best_f_score = np.inf\n",
    "best_precision = np.inf\n",
    "best_recall = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred, threshold=0.5):\n",
    "    return fbeta_score(y_true, y_pred, 2, threshold)\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta, threshold, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    y_pred = torch.ge(y_pred.float(), threshold).float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum(dim=1)\n",
    "    precision = true_positive.div(y_pred.sum(dim=1).add(eps))\n",
    "    recall = true_positive.div(y_true.sum(dim=1).add(eps))\n",
    "\n",
    "    return torch.mean(\n",
    "        (precision*recall).\n",
    "        div(precision.mul(beta2) + recall + eps).\n",
    "        mul(1 + beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaggle\n",
    "\n",
    "def F_score(logit, label, threshold=0.5, beta=2):\n",
    "    prob = torch.sigmoid(logit)\n",
    "    prob = prob > threshold\n",
    "    label = label > threshold\n",
    "\n",
    "    TP = (prob & label).sum(1).float()\n",
    "    TN = ((~prob) & (~label)).sum(1).float()\n",
    "    FP = (prob & (~label)).sum(1).float()\n",
    "    FN = ((~prob) & label).sum(1).float()\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-12)\n",
    "    recall = TP / (TP + FN + 1e-12)\n",
    "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
    "    return F2.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train | Epoch: 1/20 | train_loss:0.66014285 | f_score:0.01992434 | precision:0.02757959 | recall:0.02205206 | Time: 348.6563s\n",
      "Phase: val | Epoch: 1/20 | val_loss:0.60991658 | f_score:0.00654550 | precision:0.01705984 | recall:0.00587375 | Time: 88.6514s\n",
      "Phase: train | Epoch: 2/20 | train_loss:0.56629287 | f_score:0.06781350 | precision:0.29129662 | recall:0.05881499 | Time: 166.9354s\n",
      "Phase: val | Epoch: 2/20 | val_loss:0.51786461 | f_score:0.13845929 | precision:0.86224160 | recall:0.11667046 | Time: 43.8098s\n",
      "Phase: train | Epoch: 3/20 | train_loss:0.47883489 | f_score:0.16349665 | precision:0.87960024 | recall:0.13938395 | Time: 170.2129s\n",
      "Phase: val | Epoch: 3/20 | val_loss:0.43409168 | f_score:0.22067152 | precision:0.84883721 | recall:0.19096282 | Time: 41.2386s\n",
      "Phase: train | Epoch: 4/20 | train_loss:0.40117185 | f_score:0.22358684 | precision:0.84185342 | recall:0.19419805 | Time: 169.3039s\n",
      "Phase: val | Epoch: 4/20 | val_loss:0.36362798 | f_score:0.22319096 | precision:0.84375000 | recall:0.19336622 | Time: 40.2693s\n",
      "Phase: train | Epoch: 5/20 | train_loss:0.33748897 | f_score:0.22721217 | precision:0.83785181 | recall:0.19772713 | Time: 177.2072s\n",
      "Phase: val | Epoch: 5/20 | val_loss:0.30769531 | f_score:0.22787262 | precision:0.83963178 | recall:0.19793106 | Time: 42.7345s\n",
      "Phase: train | Epoch: 6/20 | train_loss:0.28681033 | f_score:0.22986324 | precision:0.83716939 | recall:0.20029932 | Time: 181.6064s\n",
      "Phase: val | Epoch: 6/20 | val_loss:0.26265413 | f_score:0.22722516 | precision:0.84096415 | recall:0.19724933 | Time: 44.3446s\n",
      "Phase: train | Epoch: 7/20 | train_loss:0.24718658 | f_score:0.23230400 | precision:0.83485766 | recall:0.20280038 | Time: 185.4853s\n",
      "Phase: val | Epoch: 7/20 | val_loss:0.22690829 | f_score:0.23041668 | precision:0.84024548 | recall:0.20037430 | Time: 38.0742s\n",
      "Phase: train | Epoch: 8/20 | train_loss:0.21549464 | f_score:0.23180619 | precision:0.83699980 | recall:0.20227070 | Time: 176.6550s\n",
      "Phase: val | Epoch: 8/20 | val_loss:0.19983685 | f_score:0.23051730 | precision:0.84077035 | recall:0.20040368 | Time: 45.8015s\n",
      "Phase: train | Epoch: 9/20 | train_loss:0.19063583 | f_score:0.23225405 | precision:0.83630729 | recall:0.20276164 | Time: 177.7766s\n",
      "Phase: val | Epoch: 9/20 | val_loss:0.17722847 | f_score:0.22940167 | precision:0.84265988 | recall:0.19926744 | Time: 41.4273s\n",
      "Phase: train | Epoch: 10/20 | train_loss:0.17089503 | f_score:0.23221198 | precision:0.83671310 | recall:0.20268378 | Time: 167.1674s\n",
      "Phase: val | Epoch: 10/20 | val_loss:0.15999902 | f_score:0.23028063 | precision:0.84149709 | recall:0.20015330 | Time: 36.8199s\n",
      "Phase: train | Epoch: 11/20 | train_loss:0.15472472 | f_score:0.23166908 | precision:0.83728649 | recall:0.20217620 | Time: 160.7701s\n",
      "Phase: val | Epoch: 11/20 | val_loss:0.14491074 | f_score:0.22937155 | precision:0.84253876 | recall:0.19925814 | Time: 40.8059s\n",
      "Phase: train | Epoch: 12/20 | train_loss:0.14172271 | f_score:0.23107307 | precision:0.83830204 | recall:0.20152914 | Time: 169.4483s\n",
      "Phase: val | Epoch: 12/20 | val_loss:0.13316167 | f_score:0.22874340 | precision:0.84287791 | recall:0.19865667 | Time: 40.9963s\n",
      "Phase: train | Epoch: 13/20 | train_loss:0.13107717 | f_score:0.23035454 | precision:0.83989905 | recall:0.20078162 | Time: 168.5384s\n",
      "Phase: val | Epoch: 13/20 | val_loss:0.12262974 | f_score:0.22841957 | precision:0.84318475 | recall:0.19836364 | Time: 34.2191s\n",
      "Phase: train | Epoch: 14/20 | train_loss:0.12243071 | f_score:0.22949440 | precision:0.84064607 | recall:0.19994943 | Time: 161.9535s\n",
      "Phase: val | Epoch: 14/20 | val_loss:0.11657920 | f_score:0.22897174 | precision:0.84326550 | recall:0.19887237 | Time: 47.9111s\n",
      "Phase: train | Epoch: 15/20 | train_loss:0.11518717 | f_score:0.22998681 | precision:0.84039370 | recall:0.20039770 | Time: 188.5387s\n",
      "Phase: val | Epoch: 15/20 | val_loss:0.11003040 | f_score:0.22737166 | precision:0.84418605 | recall:0.19735355 | Time: 43.4138s\n",
      "Phase: train | Epoch: 16/20 | train_loss:0.10929147 | f_score:0.22845337 | precision:0.84135070 | recall:0.19896406 | Time: 170.1984s\n",
      "Phase: val | Epoch: 16/20 | val_loss:0.10429426 | f_score:0.22815042 | precision:0.84379845 | recall:0.19806498 | Time: 36.7247s\n",
      "Phase: train | Epoch: 17/20 | train_loss:0.10504591 | f_score:0.22855596 | precision:0.84135272 | recall:0.19914016 | Time: 177.6921s\n",
      "Phase: val | Epoch: 17/20 | val_loss:0.10018380 | f_score:0.22710392 | precision:0.84479974 | recall:0.19712036 | Time: 43.4692s\n",
      "Phase: train | Epoch: 18/20 | train_loss:0.10015344 | f_score:0.23051833 | precision:0.84104987 | recall:0.20110170 | Time: 180.0790s\n",
      "Phase: val | Epoch: 18/20 | val_loss:0.09612953 | f_score:0.22935641 | precision:0.84483204 | recall:0.19935953 | Time: 42.1108s\n",
      "Phase: train | Epoch: 19/20 | train_loss:0.09690155 | f_score:0.23093041 | precision:0.84134060 | recall:0.20149936 | Time: 162.8837s\n",
      "Phase: val | Epoch: 19/20 | val_loss:0.09328499 | f_score:0.22863710 | precision:0.84534884 | recall:0.19869829 | Time: 39.7589s\n",
      "Phase: train | Epoch: 20/20 | train_loss:0.09404275 | f_score:0.23175539 | precision:0.83977993 | recall:0.20243589 | Time: 182.3335s\n",
      "Phase: val | Epoch: 20/20 | val_loss:0.09127326 | f_score:0.23046489 | precision:0.84333010 | recall:0.20046630 | Time: 45.8281s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_per_epoch = []\n",
    "recall_per_epoch = []\n",
    "f_per_epoch = []\n",
    "loss_per_epoch = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for phase in ['train', 'val']:\n",
    "        start_time = time.time()\n",
    "        if phase == 'train':\n",
    "            NeuralNet.train()\n",
    "        else:\n",
    "            NeuralNet.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_f_score = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        \n",
    "        \n",
    "        for images_batch, labels_batch in tqdm(dataloaders_dict[phase],disable = DISABLE_TQDM):\n",
    "            images_batch = images_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                pred_batch = NeuralNet(images_batch)\n",
    "                _, preds = torch.max(pred_batch.data, 1)\n",
    "                loss = loss_func(pred_batch,labels_batch)\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            labels_cpu = labels_batch.cpu().detach().numpy()\n",
    "            pred_cpu = pred_batch.cpu().detach().numpy()\n",
    "            \n",
    "#             print(metrics.multilabel_confusion_matrix(labels_cpu, pred_cpu, samplewise = true))\n",
    "            \n",
    "            running_loss += loss.item() * images_batch.size(0)\n",
    "#             running_f2_loss += F_score(labels_batch, pred_batch)\n",
    "            temp_precision, temp_recall, temp_f_score, _ = precision_recall_fscore_support(\n",
    "                                                            labels_cpu, pred_cpu > 0.5, beta=2, average='samples')\n",
    "        \n",
    "            running_precision += (temp_precision * len(images_batch))\n",
    "            running_recall += (temp_recall * len(images_batch))\n",
    "            running_f_score += (temp_f_score * len(images_batch))\n",
    "            \n",
    "            pre_per_epoch.append(temp_precision)\n",
    "            recall_per_epoch.append(temp_recall)\n",
    "            f_per_epoch.append(temp_f_score)\n",
    "            loss_per_epoch.append(loss.item())\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataloaders_dict[phase].dataset)\n",
    "        epoch_f_score = running_f_score / len(dataloaders_dict[phase].dataset)\n",
    "        epoch_precision = running_precision / len(dataloaders_dict[phase].dataset)\n",
    "        epoch_recall = running_recall / len(dataloaders_dict[phase].dataset)\n",
    "        \n",
    "        if phase == 'val' and epoch_loss < best_loss:\n",
    "#             print(\"model val_loss Improved from {:.8f} to {:.8f}\".format(best_loss,epoch_loss))\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(NeuralNet.state_dict())\n",
    "            torch.save(NeuralNet.state_dict(), weights_path)\n",
    "        \n",
    "        if phase == 'val':\n",
    "            scheduler.step(epoch_loss)\n",
    "        \n",
    "        elapsed_time = time.time()-start_time\n",
    "        print(\"Phase: {} | Epoch: {}/{} | {}_loss:{:.8f} | f_score:{:.8f} | precision:{:.8f} | recall:{:.8f} | Time: {:.4f}s\".format(phase,\n",
    "                                                                              epoch+1,\n",
    "                                                                              NUM_EPOCHS,\n",
    "                                                                              phase,\n",
    "                                                                              epoch_loss,\n",
    "                                                                              epoch_f_score,\n",
    "                                                                              epoch_precision,\n",
    "                                                                              epoch_recall,\n",
    "                                                                              elapsed_time))\n",
    "NeuralNet.load_state_dict(torch.load(best_model_wts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 21 21:02:28 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0  On |                  N/A |\n",
      "|  0%   32C    P8    18W / 250W |    787MiB / 11172MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 25%   45C    P2    56W / 250W |   3706MiB / 11172MiB |      2%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "|  0%   33C    P8    17W / 250W |   1028MiB / 11172MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 23%   42C    P2    55W / 250W |   1566MiB / 11172MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 89%   90C    P2   243W / 250W |   3407MiB / 11172MiB |     92%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "| 56%   85C    P2   152W / 250W |   5136MiB / 11172MiB |     95%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 23%   30C    P8    16W / 250W |   1924MiB / 11172MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:8A:00.0 Off |                  N/A |\n",
      "| 52%   70C    P2   153W / 250W |   5142MiB / 11172MiB |     96%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0       876      C   python                                       395MiB |\n",
      "|    0      5857      G   /usr/lib/xorg/Xorg                            50MiB |\n",
      "|    0     39038      C   ...amalpcs17/anaconda3_sockeye/bin/python3   329MiB |\n",
      "|    1     10473      C   /home1/sriparna/anaconda3/bin/python        1915MiB |\n",
      "|    1     20349      C   python3                                      485MiB |\n",
      "|    1     28135      C   python3                                      483MiB |\n",
      "|    1     32706      C   python                                       325MiB |\n",
      "|    1     44323      C   python3                                      485MiB |\n",
      "|    2     24189      C   ...ratik/anaconda2/envs/kaiser/bin/python3  1017MiB |\n",
      "|    3     13670      C   python                                       225MiB |\n",
      "|    3     20910      C   ...kamalpcs17/anaconda3_sockeye/bin/python   847MiB |\n",
      "|    3     31976      C   python                                       483MiB |\n",
      "|    4     24189      C   ...ratik/anaconda2/envs/kaiser/bin/python3  1377MiB |\n",
      "|    4     35127      C   ...ratik/anaconda2/envs/kaiser/bin/python3  2019MiB |\n",
      "|    5     39038      C   ...amalpcs17/anaconda3_sockeye/bin/python3  5125MiB |\n",
      "|    6      2606      C   /home1/sriparna/anaconda3/bin/python        1913MiB |\n",
      "|    7     39038      C   ...amalpcs17/anaconda3_sockeye/bin/python3  5131MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_cls = models.resnet50()\n",
    "resnet_cls.load_state_dict(torch.load(resnet_weights_path))\n",
    "\n",
    "class AvgPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, x.shape[2:])\n",
    "    \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self,num_outputs):\n",
    "        super(ResNet50,self).__init__()\n",
    "        self.resnet = resnet_cls\n",
    "        layer4 = self.resnet.layer4\n",
    "        self.resnet.layer4 = nn.Sequential(nn.Dropout(0.5), layer4)\n",
    "        self.resnet.avgpool = AvgPool()\n",
    "        self.resnet.fc = nn.Linear(2048, 1024)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, num_outputs)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.resnet.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        for param in self.fc1.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        for param in self.bn1.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        for param in self.fc2.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "#         for param in self.resnet.classifier.parameters():\n",
    "#             param.requires_grad = True\n",
    "            \n",
    "    def forward(self,x):\n",
    "        out = self.bn1(self.resnet(x))\n",
    "        out = F.sigmoid(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "NeuralNet = ResNet50(num_outputs = output_dim)\n",
    "NeuralNet.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool()\n",
      "    (fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      "  (1): Linear(in_features=1024, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "newmodel = torch.nn.Sequential(*(list(NeuralNet.children())[:-2]))\n",
    "print(newmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813077d7df104933927dc939a61bccf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10319), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "acc = []\n",
    "\n",
    "for img_name in tqdm(os.listdir(train_dir)):\n",
    "    img = Image.open(train_dir + '/' + img_name)\n",
    "    img = img.convert(mode = 'RGB')\n",
    "    b = transforms.ToTensor()\n",
    "    img_tensor = b(img)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    pred = newmodel(img_tensor)\n",
    "    temp = pred.detach().numpy()\n",
    "    features.append(temp)\n",
    "    acc.append(img_name.strip().split('_')[0])\n",
    "res_df = pd.DataFrame({'accession': acc,'features':features})\n",
    "\n",
    "res_df.to_pickle(f'Features/features_{voxel_type}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_env] *",
   "language": "python",
   "name": "conda-env-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
